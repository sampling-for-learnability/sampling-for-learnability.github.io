---
layout: ../layouts/Layout.astro
title: "No Regrets: Investigating and Improving Regret Approximations for Curriculum Discovery"
description: Simple project page template for your research paper, built with Astro and Tailwind CSS
favicon: Robo_Fav_Diagram.svg
thumbnail: screenshot.png
---

import Layout from "../layouts/Layout.astro";

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";

import outside from "../assets/outside.mp4";
import transformer from "../assets/transformer.webp";
import Splat from "../components/Splat.tsx"

import CodeBlock from "../components/CodeBlock.astro";
import Table from "../components/Table.astro";
export const components = {pre: CodeBlock, table: Table}

<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Alexander Rutherford",
      url: "https://amacrutherford.com",
      institution: "University of Oxford",
      notes: ["*"],
    },
    {
      name: "Michael Beukman",
      url: "https://michaelbeukman.com/",
      institution: "University of Oxford",
      notes: ["*"],
    },
    {
      name: "Timon Willi",
      url: "https://www.timonwilli.com/",
      institution: "University of Oxford",
    },
    {
      name: "Bruno Lacerda",
      url: "https://bfalacerda.github.io/",
      institution: "University of Oxford",
    },
    {
      name: "Nick Hawes",
      url: "https://www.robots.ox.ac.uk/~nickh/",
      institution: "University of Oxford",
    },
    {
      name: "Jakob Foerster",
      url: "https://www.jakobfoerster.com/",
      institution: "University of Oxford",
    }
  ]}
  conference="The Thirty-Eighth Annual Conference on Neural Information Processing Systems"
  notes={[
    {
      symbol: "*",
      text: "Equal contribution",
    },
  ]}
  links={[
    {
      name: "Paper",
      url: "https://arxiv.org/pdf/2408.15099",
      icon: "fa-solid:file-pdf",
    },
    {
      name: "SFL Algorithm",
      url: "https://github.com/amacrutherford/sampling-for-learnability",
      icon: "mdi:github",
    },
    {
      name: "JaxNav",
      url: "https://github.com/FLAIROx/JaxMARL/tree/main/jaxmarl/environments/jaxnav",
      icon: "mdi:github",
    },
    {
      name: "arXiv",
      url: "https://arxiv.org/abs/2408.15099",
      icon: "academicons:arxiv",
    },
  ]}
  />



<HighlightedSection>

## Abstract

What data or environments to use for training to improve downstream performance is a longstanding and very topical question in reinforcement learning. In particular, Unsupervised Environment Design (UED) methods have gained recent attention as their adaptive curricula promise to enable agents to be robust to in- and out-of-distribution tasks. **We investigate how existing UED methods select training environments**, focusing on task prioritisation metrics. Surprisingly, **despite methods aiming to maximise regret in theory, the practical approximations do not correlate with regret but with success rate**. As a result, a significant portion of an agent's experience comes from environments it has already mastered, offering little to no contribution toward enhancing its abilities.

Put differently, **current methods fail to predict intuitive measures of ``learnability.''** Specifically, they are unable to consistently identify those scenarios that the agent can sometimes solve, but not always. Based on our analysis, we develop a method that directly trains on scenarios with high learnability. This simple and intuitive approach outperforms existing UED methods in several binary-outcome environments, including the standard domain of Minigrid and a novel setting closely inspired by a real-world robotics problem. We further introduce a new adversarial evaluation procedure for directly measuring robustness, closely mirroring the conditional value at risk (CVaR). We open-source all our code and present visualisations of final policies here: this https URL.

</HighlightedSection>

## Regret Approximation Correlations 

As seen below, two common score functions generally correlate with success rate, and are the highest when the agent perfectly solves a level! We argue that, instead, levels with high "learnability" should be prioritised, i.e. those that are solved sometimes, but not always.

<TwoColumns>
  <Figure slot="left" caption="Max MC">
    <img src="../public/max_mc_hist.jpg">
    </img>
  </Figure>
  <Figure slot="right" caption="PVL">
    <img src="../public/pvl_hist.jpg">
    </img>
  </Figure>
</TwoColumns>


## Sampling for Learnability (SFL)

Inspired by this, we use a score function defined as _p(1-p)_. Where for a given success rate _p_ in a deterministic, binary-outcome domain, which is maximised when the agent solves the level exactly half the time. We use this to develop a new algorithm, Sampling for Learnability (SFL), which samples a large number of levels and uses rollouts to evaluate their learnability. We then train the agent on a mix of the highly learnable levels alongside random ones.

### Evaluation protocol

Furthermore, to actually test the robustness of curriculum-learned agents, we propose an adversarial evaluation procedure that better measures robustness. Mirroring Conditional Value at Risk (CVaR), it offers a more rigorous way to assess agents under challenging conditions.

### Results

Our results demonstrate that SFL outperforms multiple SoTA UED methods on several domains, including MiniGrid Maze, XLand-MiniGrid and a more real-world environment in 2D robotic navigation tasks.

<Figure>
    <img src="../public/minigrid_performance.jpeg">
    </img>
</Figure>

## JaxNav Environment

<TwoColumns>
  <Figure slot="left">
    <img src="../public/Jaxnav_MA_4.gif">
    </img>
  </Figure>
  <Figure slot="right">
    We also introduce JaxNav, a 2D geometric navigation environment for differential drive robots. Using distances readings to nearby obstacles (mimicing LiDAR readings), the direction to their goal and their current velocity, robots must navigate to their goal without colliding with obstacles. In the example shown here, there is no communication between agents, they deconflict and navigate soley based on the aforementioned features. 
  </Figure>
</TwoColumns>

## See also

[Kinetix](https://kinetix-env.github.io/), use SFL to train their general agent, with performance far exceeding other UED approaches, including Domain Randomisation!


## BibTeX citation

```bibtex
@misc{rutherford2024noregrets,
      title={No Regrets: Investigating and Improving Regret Approximations for Curriculum Discovery}, 
      author={Alexander Rutherford and Michael Beukman and Timon Willi and Bruno Lacerda and Nick Hawes and Jakob Foerster},
      year={2024},
      eprint={2408.15099},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.15099}, 
}
```